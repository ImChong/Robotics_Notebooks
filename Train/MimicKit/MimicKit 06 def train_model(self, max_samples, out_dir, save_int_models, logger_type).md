```python
        while self._sample_count < max_samples:
            train_info = self._train_iter() # learning
            
            self._sample_count = self._update_sample_count()
            output_iter = (self._iter % self._iters_per_output == 0) or (self._sample_count >= max_samples)

            if (output_iter):
                test_info = self.test_model(self._test_episodes)
            
            env_diag_info = self._env.record_diagnostics()
            self._log_train_info(train_info, test_info, env_diag_info, start_time) 
            self._logger.print_log()

            if (output_iter):
                self._logger.write_log()
                self._output_train_model(self._iter, out_model_file, int_out_dir)

                self._train_return_tracker.reset()
                self._curr_obs, self._curr_info = self._reset_envs()
            
            self._iter += 1

        return
```


# def _build_train_data(self)


好的，我用一个**最简单的案例**：**2 个环境、3 个时间步**，把每个变量的具体数值都写出来，一步一步跟着代码走。

---

## 场景设定

你在训练一个机器人站立，有 2 个并行环境（env0 和 env1），采集了 3 步。

### 原始经验数据（第 93-97 行取出）

```
时间轴 →      t=0        t=1        t=2

env0:   站着→微晃    微晃→歪了    歪了→摔倒(失败)
env1:   站着→稳住    稳住→更稳    更稳→站得好
```

对应的数据表格（每个格子 = 1 个 sample）：

|  | env0 | env1 |
|---|---|---|
| **t=0** | obs=站着, r=0.5, done=0 | obs=站着, r=0.6, done=0 |
| **t=1** | obs=微晃, r=0.3, done=0 | obs=稳住, r=0.8, done=0 |
| **t=2** | obs=歪了, r=0.0, done=1(失败) | obs=更稳, r=0.9, done=0 |

随机动作掩码 `rand_action_mask`：

|  | env0 | env1 |
|---|---|---|
| **t=0** | 1 (随机) | 1 (随机) |
| **t=1** | 0 (均值) | 1 (随机) |
| **t=2** | 1 (随机) | 0 (均值) |

数据在代码中的 shape 为 `[3, 2]`（3步 × 2环境）。

---

## 第一步：Critic 给 next_obs 打分（第 99-102 行）

把每步的 `next_obs` 送入 Critic 网络：

```
next_obs:
  t=0: [微晃, 稳住]    → Critic 打分: [8.0,  9.0]
  t=1: [歪了, 更稳]    → Critic 打分: [5.0, 10.0]
  t=2: [摔倒, 站得好]  → Critic 打分: [3.0, 10.5]
```

```
next_vals = [[8.0,  9.0],
             [5.0, 10.0],
             [3.0, 10.5]]
```

---

## 第二步：用标准答案纠正终止状态（第 104-110 行）

检查 `done` 表格，只有 **t=2, env0 的 done=1（失败）**。

`fail_val = r_fail / (1 - discount) = 0 / (1 - 0.99) = 0`

把 t=2, env0 位置的 Critic 打分 **从 3.0 改成 0**：

```
修正后 next_vals = [[8.0,  9.0],
                    [5.0, 10.0],
                    [0.0, 10.5]]   ← env0: 3.0 → 0.0
```

**为什么？** Critic 不知道 env0 在 t=2 摔倒了，它还傻乎乎地打了 3 分。但摔倒后 episode 结束了，未来价值就是 0，所以我们手动纠正。

---

## 第三步：TD(λ) 计算目标回报（第 112 行）

从最后一步往前递推（`discount=0.99`, `td_lambda=0.95`）。

先分开看两个环境：

**env0**（t=2 失败终止）：

```
t=2: new_vals[2,0] = r + γ × next_val
                   = 0.0 + 0.99 × 0.0 = 0.0
     （摔倒了，回报就是 0）

t=1: done[1,0]=0，没终止，λ生效
     new_vals[1,0] = r + γ × [(1-λ)×next_val + λ×new_vals[2,0]]
                   = 0.3 + 0.99 × [(0.05)×5.0 + (0.95)×0.0]
                   = 0.3 + 0.99 × 0.25
                   = 0.5475
     （下一步要摔了，所以这步的回报被拉得很低）

t=0: done[0,0]=0，没终止，λ生效
     new_vals[0,0] = r + γ × [(0.05)×8.0 + (0.95)×0.5475]
                   = 0.5 + 0.99 × [0.4 + 0.52]
                   = 0.5 + 0.99 × 0.92
                   = 1.41
```

**env1**（一直没终止）：

```
t=2: new_vals[2,1] = 0.9 + 0.99 × 10.5 = 11.295

t=1: new_vals[1,1] = 0.8 + 0.99 × [(0.05)×10.0 + (0.95)×11.295]
                   = 0.8 + 0.99 × [0.5 + 10.73]
                   = 0.8 + 11.12
                   = 11.92

t=0: new_vals[0,1] = 0.6 + 0.99 × [(0.05)×9.0 + (0.95)×11.92]
                   = 0.6 + 0.99 × [0.45 + 11.32]
                   = 0.6 + 11.65
                   = 12.25
```

汇总：

```
new_vals = [[ 1.41, 12.25],
            [ 0.55, 11.92],
            [ 0.00, 11.30]]
```

对比一下：env0 因为要摔倒，回报一路走低至 0；env1 站得稳，回报维持在 11-12 的高位。**很合理！**

---

## 第四步：算优势（第 114-118 行）

让 Critic 给**当前 obs**（不是 next_obs）打分：

```
vals = [[ 7.0,  8.5],    ← Critic 觉得"站着"值 7~8.5 分
        [ 6.0,  9.5],
        [ 4.0, 10.0]]
```

优势 = 实际目标回报 - Critic 的预期：

```
adv = new_vals - vals
    = [[ 1.41-7.0,  12.25-8.5 ],     [[-5.59, +3.75],
       [ 0.55-6.0,  11.92-9.5 ],  =   [-5.45, +2.42],
       [ 0.00-4.0,  11.30-10.0]]      [-4.00, +1.30]]
```

含义解读：
- **env0 全是负数**：Critic 高估了，实际情况比预期差（快摔了）
- **env1 全是正数**：Critic 低估了，实际情况比预期好（站得很稳）

---

## 第五步：优势归一化（第 120-125 行）

**只用随机动作的样本算统计量：**

```
rand_action_mask（展平）:
  t=0,env0=1  t=0,env1=1  t=1,env0=0  t=1,env1=1  t=2,env0=1  t=2,env1=0
  ✓           ✓                       ✓           ✓

adv 展平: [-5.59, +3.75, -5.45, +2.42, -4.00, +1.30]

取出随机动作的优势:  [-5.59, +3.75, +2.42, -4.00]
                       ✓      ✓            ✓     ✓

mean = (-5.59 + 3.75 + 2.42 + -4.00) / 4 = -0.855
std  = 3.87（大约）
```

归一化所有优势：

```
norm_adv = (adv - mean) / std

  env0,t=0: (-5.59 - (-0.855)) / 3.87 = -1.22
  env1,t=0: (+3.75 - (-0.855)) / 3.87 = +1.19
  env0,t=1: (-5.45 - (-0.855)) / 3.87 = -1.19
  env1,t=1: (+2.42 - (-0.855)) / 3.87 = +0.85
  env0,t=2: (-4.00 - (-0.855)) / 3.87 = -0.81
  env1,t=2: (+1.30 - (-0.855)) / 3.87 = +0.56
```

都在 [-4, 4] 范围内，不需要 clamp。

最终 PPO 用这些归一化优势来更新策略：
- **env0 的动作优势都是负的** → 策略会学着避免这些导致摔倒的动作
- **env1 的动作优势都是正的** → 策略会学着多做这些保持稳定的动作

---

## 最终全景图

```
采集的经验             Critic打分          纠正终止边界
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│obs,r,done...│───→│V(next_obs)  │───→│失败→0,成功→高│
└─────────────┘    └─────────────┘    └──────┬──────┘
                                             │
                    TD(λ) 递推 ◄──────────────┘
                        │
                        ▼
                   目标回报 new_vals ──→ 存入 tar_val (训练Critic用)
                        │
              减去 Critic(obs) 的估值
                        │
                        ▼
                   优势 adv ──→ 归一化 ──→ 存入 adv (训练Actor用)
```